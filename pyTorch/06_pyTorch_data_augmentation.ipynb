{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pyTorch_data_augmentation.ipynb","provenance":[],"authorship_tag":"ABX9TyMeRe4IYCvRzUDrMvlhUOka"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"a5wnPH0XaH-Y","colab_type":"code","colab":{}},"source":["import os\n","import zipfile\n","\n","!wget \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n","\n","with zipfile.ZipFile(\"./cats_and_dogs_filtered.zip\",\"r\") as zip_ref:\n","    zip_ref.extractall()\n","\n","base_dir = './cats_and_dogs_filtered'\n","\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PIstwYNWaTkh","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.nn import Conv2d, ReLU, Sigmoid, MaxPool2d\n","\n","import time\n","import PIL\n","import numpy as np\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","plt.ion()   # interactive mode\n","\n","# Batch size\n","bs = 20 \n","\n","# Training iteration\n","epochs = 30\n","\n","# Number of classes\n","num_classes = 2\n","\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU or CPU for training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUv0iRvWaWGD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596280222038,"user_tz":-120,"elapsed":604,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}},"outputId":"75d06d38-cc23-4c97-e58b-20607e2372fc"},"source":["# Data augmentation and normalization for training\n","# Just normalization and resizing for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize([224,224]),\n","        transforms.RandomRotation(5),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize([224,224]),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ]),\n","}\n","\n","# Pass in our transforms here\n","train_data = datasets.ImageFolder(root=train_dir, transform=data_transforms['train'])\n","validation_data = datasets.ImageFolder(root=validation_dir, transform=data_transforms['val'])\n","\n","# Get the size of our data, which will be used when calculating the average loss and accuracy\n","train_data_size = len(train_data)\n","valid_data_size = len(validation_data)\n","\n","print(train_data_size)\n","print(valid_data_size)\n","\n","# Used to import our dataset\n","trainloader = torch.utils.data.DataLoader(dataset=train_data, batch_size=bs, shuffle=True)\n","validationloader = torch.utils.data.DataLoader(dataset=validation_data, batch_size=bs, shuffle=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2000\n","1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"114pG5feau_O","colab_type":"code","colab":{}},"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.convolutional_neural_network_layers = nn.Sequential(\n","            Conv2d(in_channels=3, out_channels=16, kernel_size=(3,3)),\n","            ReLU(inplace=True),\n","            MaxPool2d(kernel_size=2),\n","\n","            Conv2d(16, 32, kernel_size=(3,3)),\n","            ReLU(),\n","            MaxPool2d(kernel_size=2),\n","\n","            Conv2d(32, 64, kernel_size=(3,3)),\n","            ReLU(),\n","            MaxPool2d(kernel_size=2)\n","        )\n","\n","        self.linear_layers = nn.Sequential(\n","            nn.Linear(43264, 10),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(10, 2),\n","            nn.Sigmoid()\n","        )\n","  \n","    def forward(self, inputs):\n","        # Pass the input tensor though each of our operations\n","        x = self.convolutional_neural_network_layers(inputs) # conv layer\n","        # Transforms the output from a convolutional layer into a dense layer\n","        size = x.shape[0]\n","        x = x.view(size, -1) # Flatten\n","        x = self.linear_layers(x) # Fully connected layer\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjbK4Wf_awDP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1596280224915,"user_tz":-120,"elapsed":366,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}},"outputId":"6ac087c8-2fbf-481a-e1a9-a03970d509e0"},"source":["model = Model()\n","model.to(device)\n","print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model(\n","  (convolutional_neural_network_layers): Sequential(\n","    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n","    (4): ReLU()\n","    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (7): ReLU()\n","    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (linear_layers): Sequential(\n","    (0): Linear(in_features=43264, out_features=10, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.2, inplace=False)\n","    (3): Linear(in_features=10, out_features=2, bias=True)\n","    (4): Sigmoid()\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2sODkmyhawWr","colab_type":"code","colab":{}},"source":["# Observe that all parameters are being optimized\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n","criterion = torch.nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lHy-V80AaxZS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"0f665512-b60e-4bbf-d9a2-12e4c3502c4e"},"source":["  train_loss, val_loss = [], []\n","\n","for epoch in range(epochs):\n","   \n","    total_train_loss = 0\n","    total_val_loss = 0\n","\n","    model.train()\n","    \n","    # training our model\n","    for idx, (image, label) in enumerate(trainloader):\n","\n","        image, label = image.to(device), label.to(device)\n","        optimizer.zero_grad()\n","        pred = model(image)\n","\n","        loss = criterion(pred, label)\n","        total_train_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","      total_train_loss = total_train_loss / (idx + 1)\n","      train_loss.append(total_train_loss)\n","    \n","    # validating our model\n","    model.eval()\n","    total = 0\n","    for idx, (image, label) in enumerate(validationloader):\n","        image, label = image.cuda(), label.cuda()\n","        pred = model(image)\n","        loss = criterion(pred, label)\n","        total_val_loss += loss.item()\n","\n","        pred = torch.nn.functional.softmax(pred, dim=1)\n","        for i, p in enumerate(pred):\n","            if label[i] == torch.max(p.data, 0)[1]:\n","                total = total + 1\n","\n","    accuracy = total / valid_data_size\n","\n","    total_val_loss = total_val_loss / (idx + 1)\n","    val_loss.append(total_val_loss)\n","\n","    if epoch % 5 == 0:\n","        print('\\nEpoch: {}/{}, Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'.format(epoch + 1, epochs, total_train_loss, total_val_loss, accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Epoch: 1/30, Train Loss: 0.6928, Val Loss: 0.6931, Val Acc: 0.5000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6KDukbtRbWn-","colab_type":"code","colab":{}},"source":["fig=plt.figure(figsize=(20, 10))\n","plt.plot(np.arange(1, epochs+1), train_loss, label=\"Train loss\")\n","plt.plot(np.arange(1, epochs+1), val_loss, label=\"Validation loss\")\n","plt.xlabel('Epoch Number')\n","plt.ylabel('Loss')\n","plt.title(\"\")\n","plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ncbpo9Z6dtB9","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}