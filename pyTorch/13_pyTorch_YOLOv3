{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"YOLOv3-PyTorch","provenance":[{"file_id":"1ntAL_zI68xfvZ4uCSAF6XT27g0U4mZbW","timestamp":1597920206672}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gwGy3RZgpvUJ","colab_type":"text"},"source":["# Our Data\n","\n","Our dataset consists of 627 vehicle images which are labeled. We have 3 directories (train, test, valid). \n","\n","The train and validation directories consist of two folders (images: in which are all of our images, labels: which have the corresponding x, y, w, h positions of where the object is located at) and a file called custom.labels in which we have all of our labels ( name of the object in the picture). \n","\n","Our test folder consists only only of images and labels which are not separated in the same way as the train and valid folder, we are only testing on this folder. \n","\n","They need to be organized in this way because of the Ultalytic's impelementation\n","\n","# Our Model\n","\n","We'll be training a YOLOv3 (You Only Look Once) model which we got from Ultralytics LLC, which is freely available for redistribution. \n","\n","The GitHub repo containing the majority of the code we'll use is available [here](https://github.com/CaLeSS0/YOLOv3).\n","\n","# Training\n","To train our model we will use `python3 train.py` script. We will set on which data to train and for how many epochs.\n","\n","# Testing\n","We'll use the `python3 detect.py` script to produce predictions. We will pass the weights as a parameter, the weights will be set to a default name, \"last.pt\" and also we are passing the location of the test images, as well as the location of the class names file. "]},{"cell_type":"markdown","metadata":{"id":"2ZCPb6MJmqif","colab_type":"text"},"source":["# Importing libraries"]},{"cell_type":"code","metadata":{"id":"KPLc9DEzIwMZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598003145127,"user_tz":-120,"elapsed":4579,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}}},"source":["import os\n","import torch"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QGT1_6H1nDZn","colab_type":"text"},"source":["# Downloading the model and the data\n","Download the model, as well as the data from our GitHub. In this file you will get all necessary code for implementing the YOLOv3 and all the data you will need for training your own YOLOv3 to recognize vehicles. "]},{"cell_type":"code","metadata":{"id":"VHS_o3KGIyXm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1598003149237,"user_tz":-120,"elapsed":8669,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}},"outputId":"c9b9edf1-12ce-43af-d02e-51764e528cdb"},"source":["!git clone https://github.com/CaLeSS0/YOLOv3.git  # clone"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'YOLOv3'...\n","remote: Enumerating objects: 1344, done.\u001b[K\n","remote: Counting objects:   0% (1/1344)\u001b[K\rremote: Counting objects:   1% (14/1344)\u001b[K\rremote: Counting objects:   2% (27/1344)\u001b[K\rremote: Counting objects:   3% (41/1344)\u001b[K\rremote: Counting objects:   4% (54/1344)\u001b[K\rremote: Counting objects:   5% (68/1344)\u001b[K\rremote: Counting objects:   6% (81/1344)\u001b[K\rremote: Counting objects:   7% (95/1344)\u001b[K\rremote: Counting objects:   8% (108/1344)\u001b[K\rremote: Counting objects:   9% (121/1344)\u001b[K\rremote: Counting objects:  10% (135/1344)\u001b[K\rremote: Counting objects:  11% (148/1344)\u001b[K\rremote: Counting objects:  12% (162/1344)\u001b[K\rremote: Counting objects:  13% (175/1344)\u001b[K\rremote: Counting objects:  14% (189/1344)\u001b[K\rremote: Counting objects:  15% (202/1344)\u001b[K\rremote: Counting objects:  16% (216/1344)\u001b[K\rremote: Counting objects:  17% (229/1344)\u001b[K\rremote: Counting objects:  18% (242/1344)\u001b[K\rremote: Counting objects:  19% (256/1344)\u001b[K\rremote: Counting objects:  20% (269/1344)\u001b[K\rremote: Counting objects:  21% (283/1344)\u001b[K\rremote: Counting objects:  22% (296/1344)\u001b[K\rremote: Counting objects:  23% (310/1344)\u001b[K\rremote: Counting objects:  24% (323/1344)\u001b[K\rremote: Counting objects:  25% (336/1344)\u001b[K\rremote: Counting objects:  26% (350/1344)\u001b[K\rremote: Counting objects:  27% (363/1344)\u001b[K\rremote: Counting objects:  28% (377/1344)\u001b[K\rremote: Counting objects:  29% (390/1344)\u001b[K\rremote: Counting objects:  30% (404/1344)\u001b[K\rremote: Counting objects:  31% (417/1344)\u001b[K\rremote: Counting objects:  32% (431/1344)\u001b[K\rremote: Counting objects:  33% (444/1344)\u001b[K\rremote: Counting objects:  34% (457/1344)\u001b[K\rremote: Counting objects:  35% (471/1344)\u001b[K\rremote: Counting objects:  36% (484/1344)\u001b[K\rremote: Counting objects:  37% (498/1344)\u001b[K\rremote: Counting objects:  38% (511/1344)\u001b[K\rremote: Counting objects:  39% (525/1344)\u001b[K\rremote: Counting objects:  40% (538/1344)\u001b[K\rremote: Counting objects:  41% (552/1344)\u001b[K\rremote: Counting objects:  42% (565/1344)\u001b[K\rremote: Counting objects:  43% (578/1344)\u001b[K\rremote: Counting objects:  44% (592/1344)\u001b[K\rremote: Counting objects:  45% (605/1344)\u001b[K\rremote: Counting objects:  46% (619/1344)\u001b[K\rremote: Counting objects:  47% (632/1344)\u001b[K\rremote: Counting objects:  48% (646/1344)\u001b[K\rremote: Counting objects:  49% (659/1344)\u001b[K\rremote: Counting objects:  50% (672/1344)\u001b[K\rremote: Counting objects:  51% (686/1344)\u001b[K\rremote: Counting objects:  52% (699/1344)\u001b[K\rremote: Counting objects:  53% (713/1344)\u001b[K\rremote: Counting objects:  54% (726/1344)\u001b[K\rremote: Counting objects:  55% (740/1344)\u001b[K\rremote: Counting objects:  56% (753/1344)\u001b[K\rremote: Counting objects:  57% (767/1344)\u001b[K\rremote: Counting objects:  58% (780/1344)\u001b[K\rremote: Counting objects:  59% (793/1344)\u001b[K\rremote: Counting objects:  60% (807/1344)\u001b[K\rremote: Counting objects:  61% (820/1344)\u001b[K\rremote: Counting objects:  62% (834/1344)\u001b[K\rremote: Counting objects:  63% (847/1344)\u001b[K\rremote: Counting objects:  64% (861/1344)\u001b[K\rremote: Counting objects:  65% (874/1344)\u001b[K\rremote: Counting objects:  66% (888/1344)\u001b[K\rremote: Counting objects:  67% (901/1344)\u001b[K\rremote: Counting objects:  68% (914/1344)\u001b[K\rremote: Counting objects:  69% (928/1344)\u001b[K\rremote: Counting objects:  70% (941/1344)\u001b[K\rremote: Counting objects:  71% (955/1344)\u001b[K\rremote: Counting objects:  72% (968/1344)\u001b[K\rremote: Counting objects:  73% (982/1344)\u001b[K\rremote: Counting objects:  74% (995/1344)\u001b[K\rremote: Counting objects:  75% (1008/1344)\u001b[K\rremote: Counting objects:  76% (1022/1344)\u001b[K\rremote: Counting objects:  77% (1035/1344)\u001b[K\rremote: Counting objects:  78% (1049/1344)\u001b[K\rremote: Counting objects:  79% (1062/1344)\u001b[K\rremote: Counting objects:  80% (1076/1344)\u001b[K\rremote: Counting objects:  81% (1089/1344)\u001b[K\rremote: Counting objects:  82% (1103/1344)\u001b[K\rremote: Counting objects:  83% (1116/1344)\u001b[K\rremote: Counting objects:  84% (1129/1344)\u001b[K\rremote: Counting objects:  85% (1143/1344)\u001b[K\rremote: Counting objects:  86% (1156/1344)\u001b[K\rremote: Counting objects:  87% (1170/1344)\u001b[K\rremote: Counting objects:  88% (1183/1344)\u001b[K\rremote: Counting objects:  89% (1197/1344)\u001b[K\rremote: Counting objects:  90% (1210/1344)\u001b[K\rremote: Counting objects:  91% (1224/1344)\u001b[K\rremote: Counting objects:  92% (1237/1344)\u001b[K\rremote: Counting objects:  93% (1250/1344)\u001b[K\rremote: Counting objects:  94% (1264/1344)\u001b[K\rremote: Counting objects:  95% (1277/1344)\u001b[K\rremote: Counting objects:  96% (1291/1344)\u001b[K\rremote: Counting objects:  97% (1304/1344)\u001b[K\rremote: Counting objects:  98% (1318/1344)\u001b[K\rremote: Counting objects:  99% (1331/1344)\u001b[K\rremote: Counting objects: 100% (1344/1344)\u001b[K\rremote: Counting objects: 100% (1344/1344), done.\u001b[K\n","remote: Compressing objects: 100% (1318/1318), done.\u001b[K\n","remote: Total 1344 (delta 28), reused 1326 (delta 17), pack-reused 0\u001b[K\n","Receiving objects: 100% (1344/1344), 19.52 MiB | 31.33 MiB/s, done.\n","Resolving deltas: 100% (28/28), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N_dkaQmirgA-","colab_type":"text"},"source":["## Set up model config\n","\n","We now need to configure our model for training. This requires editing the 'custom.data' file, which is located in the data folder, this file tells our model where to find our data, number of classes and the corresonding label names.\n","\n","Our paths for our labels and images are correct, the only thing we need to change are the class names and how many classes there are.\n","\n","Steps for making these updates are:\n","  1. Go into the data directory\n","  2. Copy all of the 'custom.lables', which is located in the train directory, content to 'data.names'\n","  3. Create a function that will read how many classes there are in the folder\n","  4. Make the modifications to the file\n","\n"]},{"cell_type":"code","metadata":{"id":"55J0O7LRuoTv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598003149241,"user_tz":-120,"elapsed":8659,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}},"outputId":"3a1d422e-2849-4f70-a192-f34edc8f51ab"},"source":["%cd /content/YOLOv3/data"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/YOLOv3/data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kq-kLpfNlZTQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598003152368,"user_tz":-120,"elapsed":11782,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}}},"source":["# Convert .labels to .names for Ultralytics specification\n","%cat ../train/custom.labels > ../train/data.names"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"gol1kFpN0FQh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598003152374,"user_tz":-120,"elapsed":11784,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}}},"source":["def num_classes(file_path):\n","    counter = 0\n","\n","    with open(file_path) as readed_file:\n","\n","      for line in readed_file:\n","        counter += 1\n","\n","    return counter"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"yG4X2vQh0rZf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598003152381,"user_tz":-120,"elapsed":11788,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}}},"source":["# update the roboflow.data file with correct number of classes\n","import re\n","\n","num_classes = num_classes(\"../train/custom.labels\")\n","with open(\"custom.data\") as f:\n","    s = f.read()\n","\n","with open(\"custom.data\", 'w') as f:\n","    \n","    # Set number of classes num_classes.\n","    s = re.sub('classes=[0-9]+',\n","               'classes={}'.format(num_classes), s)\n","    f.write(s)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Kxk65sDpyOT","colab_type":"text"},"source":["After modifying the file we can check how it's content looks like.\n","\n","We can see that the content consists of the number of classes, traing and validation directory, as well as the file for the labels."]},{"cell_type":"code","metadata":{"id":"un6Or-F31M1Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1598003155860,"user_tz":-120,"elapsed":15255,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}},"outputId":"90772b97-db8c-4f83-e092-3ae7c3f10c12"},"source":["# display updated number of classes\n","%cat custom.data"],"execution_count":7,"outputs":[{"output_type":"stream","text":["classes=5\n","train=train/images/train_images.txt\n","valid=valid/images/valid_images.txt\n","names=train/data.names\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iqOa_ogHqhOg","colab_type":"text"},"source":["# Training our model\n","\n","Once we have our data prepped, we'll train our model using the training script mentioned earlier.\n","\n","We need to move one directory bellow using this command:"]},{"cell_type":"code","metadata":{"id":"3pEzmWztuevf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598003155865,"user_tz":-120,"elapsed":15246,"user":{"displayName":"Strahinja Stefanovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLc0028AcX-qbTBPemQDnnhu36BhcnROR1OzG=s64","userId":"02398161038051326150"}},"outputId":"6dd8fdd1-d6b9-46c5-ce9d-5a6b5748db80"},"source":["%cd ../"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/YOLOv3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xhBZig5QqQps","colab_type":"text"},"source":["Now we call the train.py script and pass the path to the file which has the information where our files are located. We will train for 50 epochs."]},{"cell_type":"code","metadata":{"id":"gnVZo1BknVC_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"25675cb9-fcb6-42e5-cebe-4efef1dac3d8"},"source":["!python3 train.py --data data/custom.data --epochs 50"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Namespace(accumulate=4, adam=False, batch_size=16, bucket='', cache_images=False, cfg='cfg/yolov3-spp.cfg', data='data/custom.data', device='', epochs=50, evolve=False, img_size=[416], multi_scale=False, name='', nosave=False, notest=False, rect=False, resume=False, single_cls=False, var=None, weights='weights/yolov3-spp-ultralytics.pt')\n","Using CUDA device0 _CudaDeviceProperties(name='Tesla T4', total_memory=15079MB)\n","\n","2020-08-21 10:08:15.884854: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","Model Summary: 225 layers, 6.29987e+07 parameters, 6.29987e+07 gradients\n","Caching labels (439 found, 0 missing, 0 empty, 0 duplicate, for 439 images): 100% 439/439 [00:00<00:00, 1947.94it/s]\n","Caching labels (125 found, 0 missing, 0 empty, 0 duplicate, for 125 images): 100% 125/125 [00:00<00:00, 1676.92it/s]\n","Starting training for 50 epochs...\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","  0% 0/28 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n","  FutureWarning)\n","      0/49     8.22G     0.591      1.72      3.92      6.23        12       416: 100% 28/28 [00:33<00:00,  1.19s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1:   0% 0/4 [00:00<?, ?it/s]/content/YOLOv3/utils/utils.py:532: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n","  i, j = (pred[:, 5:] > conf_thres).nonzero().t()\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:05<00:00,  1.29s/it]\n","                 all       125       227     0.232     0.522     0.322     0.296\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","  0% 0/28 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n","  FutureWarning)\n","      1/49     9.64G     0.598      1.61      1.83      4.04        26       416: 100% 28/28 [00:31<00:00,  1.14s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:03<00:00,  1.28it/s]\n","                 all       125       227     0.168     0.758     0.346     0.268\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","      2/49     9.64G     0.633      1.43      1.29      3.35        18       416: 100% 28/28 [00:34<00:00,  1.22s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:03<00:00,  1.17it/s]\n","                 all       125       227     0.166     0.803     0.348     0.246\n","\n","Model Bias Summary:    layer        regression        objectness    classification\n","                          89      -0.15+/-0.33      -3.08+/-1.35      -6.57+/-0.90 \n","                         101       0.02+/-0.25      -2.97+/-0.16      -6.33+/-0.68 \n","                         113       0.08+/-0.32      -3.88+/-0.63      -6.03+/-0.72 \n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","      3/49     9.64G     0.563      1.32      1.15      3.04        15       416: 100% 28/28 [00:34<00:00,  1.22s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:03<00:00,  1.23it/s]\n","                 all       125       227     0.269     0.679     0.348     0.322\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","      4/49     9.64G     0.599      1.32      1.02      2.94        17       416: 100% 28/28 [00:33<00:00,  1.21s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:03<00:00,  1.23it/s]\n","                 all       125       227     0.256     0.749     0.399     0.346\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","      5/49     9.64G     0.595      1.31     0.814      2.71        21       416: 100% 28/28 [00:34<00:00,  1.23s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:03<00:00,  1.21it/s]\n","                 all       125       227     0.216     0.774      0.46     0.312\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","      6/49     9.64G     0.572      1.15     0.774       2.5        14       416: 100% 28/28 [00:33<00:00,  1.21s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:03<00:00,  1.23it/s]\n","                 all       125       227     0.284     0.739     0.479     0.382\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","      7/49     9.64G     0.565      1.14     0.664      2.37        20       416: 100% 28/28 [00:34<00:00,  1.23s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:03<00:00,  1.23it/s]\n","                 all       125       227     0.373      0.72     0.516     0.461\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","      8/49     9.64G     0.553      1.08     0.531      2.16        18       416: 100% 28/28 [00:34<00:00,  1.22s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:03<00:00,  1.22it/s]\n","                 all       125       227     0.364     0.701     0.499     0.463\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","      9/49     9.64G     0.537      1.17     0.546      2.25        14       416: 100% 28/28 [00:34<00:00,  1.22s/it]\n","               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 4/4 [00:03<00:00,  1.22it/s]\n","                 all       125       227     0.388     0.629       0.5     0.458\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","  0% 0/28 [00:00<?, ?it/s]"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ubWkZfNK6RCk","colab_type":"text"},"source":["### Testing\n","\n","After training our data we can now proceed to the testing phase. We will use the script mentioned earlier and pass a couple of parameters.\n","\n","- Weights: we're specifying the weights to use for our model, after training our model has saved the weights by the name 'last.pt'\n","- Source: we're specifying the path to the images we want to use for testing\n","- Names: we're defining the labels we want to use."]},{"cell_type":"code","metadata":{"id":"FB4Lfxt-n0qP","colab_type":"code","colab":{}},"source":["!python3 detect.py --weights weights/last.pt --source=test --names=train/data.names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jGgVRiJd8HJZ","colab_type":"text"},"source":["### Displaying our results\n","\n","Ultralytic's YOLOv3 generates predictions which include the labels and bounding boxes shown directly on top of our images. They're saved in our output directory.\n","\n","We are now going to show the results by plotting the images"]},{"cell_type":"code","metadata":{"id":"SdAEEulm4zxI","colab_type":"code","colab":{}},"source":["# import libraries for display\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzxLXker5dgx","colab_type":"code","colab":{}},"source":["# grab all images from our output directory\n","images = [ Image.open('./output/' + f) for f in os.listdir('./output') ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pjcEs32q5EYz","colab_type":"code","colab":{}},"source":["fig = plt.figure(figsize=(10, 8))\n","fig.subplots_adjust(left=0, right=1, bottom=-1, top=0)\n","\n","cnt = 0\n","for i in range(5, 14):\n","  ax = fig.add_subplot(3, 3, cnt + 1, xticks=[], yticks=[])\n","  ax.imshow(images[i])\n","  cnt += 1\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6cTWdXC1biUQ","colab_type":"text"},"source":["https://github.com/roboflow-ai/yolov3\n","https://github.com/ultralytics/yolov3"]}]}